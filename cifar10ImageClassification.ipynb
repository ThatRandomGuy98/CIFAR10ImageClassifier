{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159c915e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from skorch import NeuralNetClassifier\n",
    "import itertools\n",
    "import torch.optim as optim\n",
    "# from torchinfo import summary\n",
    "# import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"NVIDIA GPU available, running stuff on GPU\" if DEVICE.type == \"cuda\" else \"No NVIDIA GPU available, running stuff on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f56597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = r\"C:\\Users\\delga\\Documents\\programming\\datasets\"\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616])  #Known metrics from CIFAR10\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616])   #Known metrics from CIFAR10\n",
    "])\n",
    "\n",
    "full_train_dataset = datasets.CIFAR10(\n",
    "    root=ROOT_DIR, \n",
    "    download=False, \n",
    "    train=True, \n",
    "    transform=train_transforms\n",
    "    )\n",
    "\n",
    "train_size = int(0.9 * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset=full_train_dataset,\n",
    "    lengths=[train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(22)\n",
    ")\n",
    "\n",
    "val_dataset.dataset.transform = val_transforms      # Replace val transform (so validation doesn't use augmentations)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root=ROOT_DIR, \n",
    "    download=False, \n",
    "    train=False, \n",
    "    transform=val_transforms\n",
    "    )\n",
    "\n",
    "print(f\"\\nCompleted data loading and splits:\\n * Train dataset: {len(train_dataset)} samples\\n * Validation dataset: {len(val_dataset)} samples\\n * Test dataset: {len(test_dataset)} samples\")\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True, num_workers=1, pin_memory=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=128, shuffle=True, num_workers=1, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=128, shuffle=True, num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321f697a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = torch.arange(100)\n",
    "# tiny_train_dataset = Subset(dataset=train_dataset, indices=indices)\n",
    "# tiny_loader = DataLoader(dataset=tiny_train_dataset, batch_size=16, shuffle=True)\n",
    "# images, labels = next(iter(tiny_loader))\n",
    "# print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bc89bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Convolutional Neural Network for CIFAR-10 classification.\n",
    "    Input: 3x32x32 images\n",
    "    Output: 10 class logits\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout=0.25):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # --- Convolutional layers ---\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        \n",
    "        # --- Batch Normalization layers + Pooling layer ---\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=32)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=64)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=128)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # --- Fully Connected layers ---\n",
    "        self.fc1 = nn.Linear(in_features=128 * 4 * 4, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=10)  # 10 CIFAR-10 classes\n",
    "        \n",
    "        # --- Regularization ---\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Convolutional feature extraction\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))   # Output: [B, 32, 16, 16]\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))   # Output: [B, 64, 8, 8]\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))   # Output: [B, 128, 4, 4]\n",
    "\n",
    "        # --- Flatten for fully connected layers ---\n",
    "        x = torch.flatten(x, 1)  # Flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c027639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(DEVICE)\n",
    "# summary(model, input_size=(16, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef07278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, train_loader, val_loader, epochs, criterion, optimizer):\n",
    "    \"\"\"\n",
    "    Trains the model {epochs} number of times, within each epoch it\n",
    "    trains the whole data inside the loader and later evaluates it using\n",
    "    the validation loader\n",
    "    \"\"\"\n",
    "    train_losses, val_losses, val_accuracies = [], [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # --- Training phase ---\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # --- Evaluation phase ---\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_val_loss += loss.item()\n",
    "                \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        val_accuracy = 100 * correct / total\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f} | \"\n",
    "              f\"Val Acc: {val_accuracy:.2f}%\")\n",
    "    \n",
    "    # --- Plotting the curves ---    \n",
    "    epochs_range = range(1, epochs + 1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs_range, val_losses, label=\"Validation Loss\")\n",
    "    plt.title(\"Loss Curves\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, val_accuracies, label=\"Validation Accuracy\", color=\"green\")\n",
    "    plt.title(\"Validation Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return train_losses, val_losses, val_accuracies\n",
    "          \n",
    "        \n",
    "def test_loop(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"\\nTest Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e58413",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [1e-2, 5e-3, 1e-3]\n",
    "weight_decays = [1e-3, 1e-4, 1e-5]\n",
    "dropouts = [0.25, 0.3, 0.4]\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "best_validation_accuracy = 0\n",
    "best_params = None\n",
    "best_model_state = None\n",
    "\n",
    "for lr, wd, drop in itertools.product(learning_rates, weight_decays, dropouts):\n",
    "    print(f\"\\n---Testing configuration: learning rate={lr}, weight decay={wd}, dropout={drop}\")\n",
    "    model = CNN(dropout=drop).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    _, _, val_accuracies = train_and_validate(model, train_loader, val_loader, epochs=5, criterion=criterion, optimizer=optimizer)\n",
    "    val_acc = val_accuracies[-1]\n",
    "    \n",
    "    if val_acc > best_validation_accuracy:\n",
    "        best_validation_accuracy = val_acc\n",
    "        best_params = (lr, wd, drop)\n",
    "        best_model_state = model.state_dict()\n",
    "        \n",
    "print(f\"\\nBest hyperparameters found:\")\n",
    "print(f\"   Learning rate: {best_params[0]}\")\n",
    "print(f\"   Weight decay:  {best_params[1]}\")\n",
    "print(f\"   Dropout:       {best_params[2]}\")\n",
    "print(f\"   Validation accuracy: {best_validation_accuracy:.2f}%\")\n",
    "\n",
    "# --- Using the best found parameters for the testing phase ---\n",
    "best_model = CNN(dropout=best_params[2]).to(DEVICE)\n",
    "best_model.load_state_dict(best_model_state)\n",
    "test_loop(best_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb568efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Improve current CNN\n",
    "Tune training hyperparameters\n",
    "Use a pre trained model (transfer learning)\n",
    "train on CIFAR100\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
